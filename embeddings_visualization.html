
        <html>
        <head>
            <meta charset="utf-8" />
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
            </style>
        </head>
        <body>
            <html>
<head><meta charset="utf-8" /></head>
<body>
    <div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.0.1.min.js" integrity="sha256-oy6Be7Eh6eiQFs5M7oXuPxxm9qbJXEtTpfSI93dW16Q=" crossorigin="anonymous"></script>                <div id="myplot" class="plotly-graph-div" style="height:800px; width:1200px;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("myplot")) {                    Plotly.newPlot(                        "myplot",                        [{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#636EFA","opacity":0.7,"size":8},"mode":"markers","name":"DEPLOYMENT.md","text":["File: DEPLOYMENT.md\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: # Voice Assistant Deployment Guide\n\nThis guide explains how to set up automated deployments with GitHub webhooks, Docker, and zero-downtime blue-green deployments.\n\n## Architecture Overview\n\nThe deplo...","File: DEPLOYMENT.md\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: ration (default: `false`)\n\n### 4. GitHub Repository Setup\n\n#### Add Secrets\n\nIn your GitHub repository, go to Settings \u003e Secrets and add:\n- `WEBHOOK_URL`: `http:\u002f\u002fyour-server-ip:9000\u002fhooks\u002fgithub-depl...","File: DEPLOYMENT.md\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: rocess\n\nThe deployment script follows these steps:\n\n1. **Determine Colors**: Identifies active (blue\u002fgreen) and target colors\n2. **Pull Image**: Downloads the new Docker image\n3. **Start Container**: ...","File: DEPLOYMENT.md\u003cbr\u003eChunk: 3\u003cbr\u003ePreview: ode for personal deployments where you want to prevent new registrations.\n\n### Enabling Single User Mode\n\n1. Set `SINGLE_USER_MODE=true` in your `.env.production.local`\n2. Restart the application\n\nWhe...","File: DEPLOYMENT.md\u003cbr\u003eChunk: 4\u003cbr\u003ePreview: es: 3\n  start_period: 40s\n```\n\n### Deployment Timeouts\n\nAdjust in `scripts\u002fdeploy.sh`:\n\n```bash\nHEALTH_CHECK_RETRIES=30  # Number of attempts\nHEALTH_CHECK_DELAY=2     # Seconds between attempts\n```\n\n#..."],"x":[8.480762481689453,8.15096378326416,7.394402027130127,6.42981481552124,6.896230220794678],"y":[-1.2407156229019165,-1.3763391971588135,-1.2275185585021973,-1.2021089792251587,-2.717602014541626],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#EF553B","opacity":0.7,"size":8},"mode":"markers","name":"docker-compose.production.yml","text":["File: docker-compose.production.yml\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: version: '3.8'\n\nservices:\n  voice-assistant-blue:\n    image: ghcr.io\u002f${GITHUB_REPOSITORY}:${DEPLOY_TAG:-latest}\n    container_name: voice-assistant-blue\n    ports:\n      - \"8001:8000\"\n    volumes:\n   ...","File: docker-compose.production.yml\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: - WEBHOOK_SECRET=${WEBHOOK_SECRET}\n      - GITHUB_REPOSITORY=${GITHUB_REPOSITORY}\n    restart: unless-stopped\n    networks:\n      - voice-network\n\nnetworks:\n  voice-network:\n    driver: bridge"],"x":[8.858467102050781,9.608603477478027],"y":[-2.9233181476593018,-3.2516441345214844],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#00CC96","opacity":0.7,"size":8},"mode":"markers","name":"README.md","text":["File: README.md\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: # Voice Assistant\n\nA simple, self-hosted voice assistant application with push-to-talk functionality, built with Python (FastAPI) and vanilla JavaScript.\n\n## Features\n\n- **Push-to-Talk Interface**: Ho...","File: README.md\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: dler.py    # AI reasoning and tools\n\u2502   \u2514\u2500\u2500 tts_instructor.py # Dynamic TTS instruction generation\n\u251c\u2500\u2500 frontend\u002f            # Static HTML\u002fJS frontend\n\u2502   \u251c\u2500\u2500 index.html       # Main app page\n\u2502   \u251c\u2500\u2500 l...","File: README.md\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: nical, anxious, etc.)\n- Provides more natural and empathetic responses\n- Only works with the `gpt-4o-mini-tts` model\n\nTo disable this feature, set `ENABLE_DYNAMIC_TTS_INSTRUCTIONS=false` in your `.env..."],"x":[2.9170641899108887,2.8862671852111816,1.8423813581466675],"y":[-3.351806640625,-3.8128578662872314,-5.409478664398193],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#AB63FA","opacity":0.7,"size":8},"mode":"markers","name":"SPECIFICATION.md","text":["File: SPECIFICATION.md\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: # Voice Assistant Application Specification\n\n## Project Overview\n\nA simple, self-hosted voice assistant application with push-to-talk functionality, designed for both desktop and mobile browsers. The ...","File: SPECIFICATION.md\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: \u002fapi\u002fauth\u002flogout\nHeaders: Authorization: Bearer \u003ctoken\u003e\nResponse: { \"message\": \"Logged out successfully\" }\n```\n\n### Voice Processing\n```\nPOST \u002fapi\u002fvoice\u002fprocess\nHeaders: Authorization: Bearer \u003ctoken\u003e\n...","File: SPECIFICATION.md\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: sion Model\n```python\nclass Session(BaseModel):\n    token: str\n    user_id: str\n    created_at: datetime\n    expires_at: datetime\n```\n\n## File Structure\n```\nvoice\u002f\n\u251c\u2500\u2500 backend\u002f\n\u2502   \u251c\u2500\u2500 app.py          ...","File: SPECIFICATION.md\u003cbr\u003eChunk: 3\u003cbr\u003ePreview: ts\n\n## Deployment\n\n### Docker Configuration\n```dockerfile\nFROM python:3.11-slim\nWORKDIR \u002fapp\nCOPY backend\u002frequirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY backend\u002f .\u002fbackend\u002f..."],"x":[1.9851139783859253,0.15458202362060547,3.092085123062134,3.9712343215942383],"y":[-2.128816604614258,-1.381532073020935,-1.884555459022522,-2.015960693359375],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#FFA15A","opacity":0.7,"size":8},"mode":"markers","name":"app_flow.md","text":["File: app_flow.md\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: # Voice Assistant Application - Technical Flow Documentation\n\n## Table of Contents\n1. [System Architecture Overview](#system-architecture-overview)\n2. [Frontend Technical Flow](#frontend-technical-flo...","File: app_flow.md\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: g, MP3 playback, OpenAI STT\u002fTTS\n\n## Frontend Technical Flow\n\n### 1. Application Initialization\n\n#### Authentication Check (app.js:2-5)\n```javascript\nconst token = localStorage.getItem('access_token');...","File: app_flow.md\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: sage\n\n#### Conversation Management\n- Load conversations on startup\n- Active conversation highlighting\n- Click-to-load previous conversations\n- Real-time conversation list updates\n\n### 5. Exercise Syst...","File: app_flow.md\u003cbr\u003eChunk: 3\u003cbr\u003ePreview: low (ai_handler.py)\n1. **Context Building**:\n   - Extract system message (if exercise)\n   - Include last 10 messages for context\n   - Add current user input\n\n2. **Tool Execution**:\n   - Check if AI re...","File: app_flow.md\u003cbr\u003eChunk: 4\u003cbr\u003ePreview: \"JWT_TOKEN\",\n    \"token_type\": \"bearer\"\n}\n```\n\n### Voice Processing\n\n#### POST \u002fapi\u002fvoice\u002fprocess\n**Request:**\n- Multipart form data\n- Field: `audio` - WebM audio file\n\n**Response:**\n```json\n{\n    \"tr...","File: app_flow.md\u003cbr\u003eChunk: 5\u003cbr\u003ePreview: ecode_token(credentials.credentials)\n    user = load_user_by_id(payload[\"sub\"])\n    if not user:\n        raise HTTPException(401, \"Invalid authentication\")\n    return user\n```\n\n## Data Persistence\n\n##..."],"x":[1.8798346519470215,1.4852566719055176,0.9129767417907715,-0.5904251337051392,0.7652658820152283,3.0100455284118652],"y":[-0.8068982362747192,-0.0543244332075119,0.389478474855423,-2.059314727783203,-1.404245138168335,-0.5520116686820984],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#19D3F3","opacity":0.7,"size":8},"mode":"markers","name":"docker-compose.yml","text":["File: docker-compose.yml\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: version: '3.8'\n\nservices:\n  voice-assistant:\n    build: .\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - .\u002fdata:\u002fapp\u002fdata\n      - .\u002faudio:\u002fapp\u002faudio\n    env_file:\n      - .env\n    environment:\n  ..."],"x":[8.390344619750977],"y":[-3.916865825653076],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#FF6692","opacity":0.7,"size":8},"mode":"markers","name":".github\u002fworkflows\u002fdeploy.yml","text":["File: .github\u002fworkflows\u002fdeploy.yml\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: name: Build and Deploy\n\non:\n  push:\n    branches: [ main ]\n  workflow_dispatch:\n\nenv:\n  REGISTRY: ghcr.io\n  IMAGE_NAME: ${{ github.repository }}\n\njobs:\n  build-and-push:\n    runs-on: ubuntu-latest\n   ..."],"x":[9.771361351013184],"y":[-0.3336782455444336],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#B6E880","opacity":0.7,"size":8},"mode":"markers","name":"docs\u002fexercises.md","text":["File: docs\u002fexercises.md\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: # Voice Assistant Exercises\n\n## Overview\n\nThe Voice Assistant now includes an interactive exercise system that provides structured learning experiences through guided conversations. Users can select f...","File: docs\u002fexercises.md\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: l Thinking\n8. **Devil's Advocate Debate** (Intermediate, 15-20 min)\n   - Defend viewpoints against logical challenges\n   - Strengthen argumentation skills\n\n## Technical Implementation\n\n### Data Storag...","File: docs\u002fexercises.md\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: st entertainment"],"x":[0.01602034457027912,-0.11829466372728348,2.768531560897827],"y":[2.391573667526245,2.584144115447998,-10.735772132873535],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#FF97FF","opacity":0.7,"size":8},"mode":"markers","name":"docs\u002fautomatic_voice_prefer...","text":["File: docs\u002fautomatic_voice_preferences.md\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: # Automatic Voice Preference Learning\n\n## Overview\n\nThe voice assistant now automatically learns how each user prefers to be spoken to by analyzing natural conversation feedback. This feature eliminat...","File: docs\u002fautomatic_voice_preferences.md\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: automatically migrated on first login\n\n### Privacy\n- Preferences are stored locally with user data\n- No voice preferences are shared between users\n- Each user's experience is completely personalized\n\n..."],"x":[0.7509718537330627,1.1374365091323853],"y":[-6.639222621917725,-6.470303535461426],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#FECB52","opacity":0.7,"size":8},"mode":"markers","name":"docs\u002fspeech_to_text.md","text":["File: docs\u002fspeech_to_text.md\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: Speech to text\n==============\n\nLearn how to turn audio into text.\n\nThe Audio API provides two speech to text endpoints:\n\n*   `transcriptions`\n*   `translations`\n\nHistorically, both endpoints have been...","File: docs\u002fspeech_to_text.md\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: o\u002ffile\u002faudio.mp3 \\\n  --form model=gpt-4o-transcribe\n```\n\nBy default, the response type will be json with the raw text included.\n\n{ \"text\": \"Imagine the wildest idea that you've ever had, and you're cu...","File: docs\u002fspeech_to_text.md\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: o into English. This differs from our \u002fTranscriptions endpoint since the output is not in the original input language and is instead translated to English text. This endpoint supports only the `whispe...","File: docs\u002fspeech_to_text.md\u003cbr\u003eChunk: 3\u003cbr\u003ePreview: as trained on 98 languages, we only list the languages that exceeded \u003c50% [word error rate](https:\u002f\u002fen.wikipedia.org\u002fwiki\u002fWord_error_rate) (WER) which is an industry standard benchmark for speech to t...","File: docs\u002fspeech_to_text.md\u003cbr\u003eChunk: 4\u003cbr\u003ePreview: mat=\"verbose_json\"\n```\n\nThe `timestamp_granularities[]` parameter is only supported for `whisper-1`.\n\nLonger inputs\n-------------\n\nBy default, the Transcriptions API only supports files that are less ...","File: docs\u002fspeech_to_text.md\u003cbr\u003eChunk: 5\u003cbr\u003ePreview: io_file, \n  response_format=\"text\",\n  prompt=\"The following conversation is a lecture about the recent developments around OpenAI, GPT-4.5 and the future of AI.\"\n)\n\nprint(transcription.text)\n```\n\n```b...","File: docs\u002fspeech_to_text.md\u003cbr\u003eChunk: 6\u003cbr\u003ePreview: t includes punctuation: \"Hello, welcome to my lecture.\"\n4.  The model may also leave out common filler words in the audio. If you want to keep the filler words in your transcript, use a prompt that co...","File: docs\u002fspeech_to_text.md\u003cbr\u003eChunk: 7\u003cbr\u003ePreview: fs.createReadStream(\"\u002fpath\u002fto\u002ffile\u002fspeech.mp3\"),\n  model: \"gpt-4o-mini-transcribe\",\n  response_format: \"text\",\n  stream: true,\n});\n\nfor await (const event of stream) {\n  console.log(event);\n}\n```\n\n```...","File: docs\u002fspeech_to_text.md\u003cbr\u003eChunk: 8\u003cbr\u003ePreview: \"prefix_padding_ms\": 300,\n    \"silence_duration_ms\": 500,\n  },\n  \"input_audio_noise_reduction\": {\n    \"type\": \"near_field\"\n  },\n  \"include\": [\n    \"item.input_audio_transcription.logprobs\"\n  ]\n}\n```\n\n...","File: docs\u002fspeech_to_text.md\u003cbr\u003eChunk: 9\u003cbr\u003ePreview: lves using the optional prompt parameter to pass a dictionary of the correct spellings.\n\nBecause it wasn't trained with instruction-following techniques, Whisper operates more like a base GPT model. K...","File: docs\u002fspeech_to_text.md\u003cbr\u003eChunk: 10\u003cbr\u003ePreview: ble. Similar to what we did with the prompt parameter earlier, we can define our company and product names.\n\nPost-processing\n\n```javascript\nconst systemPrompt = `\nYou are a helpful assistant for the c...","File: docs\u002fspeech_to_text.md\u003cbr\u003eChunk: 11\u003cbr\u003ePreview: t(\n  0, system_prompt, fake_company_filepath\n)\n```\n\nIf you try this on your own audio file, you'll see that GPT-4 corrects many misspellings in the transcript. Due to its larger context window, this m..."],"x":[-4.170078754425049,-3.9439096450805664,-4.957709312438965,-3.991000175476074,-3.070155382156372,-2.34446382522583,-2.2607336044311523,-2.628751754760742,-1.5624388456344604,-0.5374826788902283,-0.02349471114575863,0.46230942010879517],"y":[10.07916259765625,10.479427337646484,11.0762357711792,11.757702827453613,12.101572036743164,11.467294692993164,10.533268928527832,9.47493839263916,9.765701293945312,11.718985557556152,12.051615715026855,12.476325035095215],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#636EFA","opacity":0.7,"size":8},"mode":"markers","name":"docs\u002ftext_to_speech.md","text":["File: docs\u002ftext_to_speech.md\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: Text to speech\n==============\n\nLearn how to turn text into lifelike spoken audio.\n\nThe Audio API provides a [`speech`](\u002fdocs\u002fapi-reference\u002faudio\u002fcreateSpeech) endpoint based on our [GPT-4o mini TTS (t...","File: docs\u002ftext_to_speech.md\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: a cheerful and positive tone.\",\n) as response:\n    response.stream_to_file(speech_file_path)\n```\n\n```bash\ncurl https:\u002f\u002fapi.openai.com\u002fv1\u002faudio\u002fspeech \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n ...","File: docs\u002ftext_to_speech.md\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: played before the full file is generated and made accessible.\n\nStream spoken audio from input text directly to your speakers\n\n```javascript\nimport OpenAI from \"openai\";\nimport { playAudio } from \"open...","File: docs\u002ftext_to_speech.md\u003cbr\u003eChunk: 3\u003cbr\u003ePreview: wav` or `pcm` as the response format.\n\nSupported output formats\n------------------------\n\nThe default response format is `mp3`, but other formats like `opus` and `wav` are available.\n\n*   **MP3**: The...","File: docs\u002ftext_to_speech.md\u003cbr\u003eChunk: 4\u003cbr\u003ePreview: ng to them."],"x":[-3.8878252506256104,-3.9968745708465576,-4.766626358032227,-5.931790351867676,1.5389996767044067],"y":[8.7746000289917,8.31943416595459,8.750699043273926,9.147330284118652,-10.547798156738281],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#EF553B","opacity":0.7,"size":8},"mode":"markers","name":"frontend\u002fstyle.css","text":["File: frontend\u002fstyle.css\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: * {\n    margin: 0;\n    padding: 0;\n    box-sizing: border-box;\n}\n\nbody {\n    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;\n    background-c...","File: frontend\u002fstyle.css\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: .header-buttons {\n    display: flex;\n    gap: 10px;\n}\n\n.logout-button {\n    padding: 8px 16px;\n    background-color: #e74c3c;\n    color: white;\n    border: none;\n    border-radius: 5px;\n    cursor: po...","File: frontend\u002fstyle.css\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: nt-weight: bold;\n    margin-bottom: 5px;\n    font-size: 14px;\n    color: #666;\n}\n\n.message-content {\n    margin-bottom: 5px;\n}\n\n.tool-calls {\n    margin-top: 10px;\n    padding-top: 10px;\n    border-to...","File: frontend\u002fstyle.css\u003cbr\u003eChunk: 3\u003cbr\u003ePreview: cursor: pointer;\n    color: #666;\n    padding: 0;\n    width: 30px;\n    height: 30px;\n    display: flex;\n    align-items: center;\n    justify-content: center;\n}\n\n.close-button:hover {\n    color: #333;\n...","File: frontend\u002fstyle.css\u003cbr\u003eChunk: 4\u003cbr\u003ePreview: or: #999;\n}\n\n\u002f* Active Exercise Banner *\u002f\n.active-exercise-banner {\n    position: fixed;\n    top: 0;\n    left: 0;\n    right: 0;\n    background: #9b59b6;\n    color: white;\n    padding: 10px 20px;\n    d..."],"x":[5.322793483734131,4.736476421356201,3.9327025413513184,3.641735076904297,4.423106670379639],"y":[6.124578475952148,5.85498571395874,6.541574954986572,7.457594871520996,7.318884372711182],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#00CC96","opacity":0.7,"size":8},"mode":"markers","name":"frontend\u002fapp.js","text":["File: frontend\u002fapp.js\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: \u002f\u002f Check authentication\nconst token = localStorage.getItem('access_token');\nif (!token) {\n    window.location.href = '\u002flogin.html';\n}\n\n\u002f\u002f API helper\nasync function apiCall(endpoint, options = {}) {\n  ...","File: frontend\u002fapp.js\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: \u002f\u002f Update conversation\n        currentConversationId = data.conversation_id;\n        \n        \u002f\u002f Display messages\n        displayMessage('user', data.transcription);\n        displayMessage('assistant'...","File: frontend\u002fapp.js\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: lDiv.textContent = `\ud83d\udd27 ${toolCall.name}: ${JSON.stringify(toolCall.result)}`;\n            toolCallsDiv.appendChild(toolDiv);\n        });\n        \n        messageDiv.appendChild(toolCallsDiv);\n    }\n   ...","File: frontend\u002fapp.js\u003cbr\u003eChunk: 3\u003cbr\u003ePreview: ventListener('click', () =\u003e loadConversation(conv.id));\n            \n            conversationList.appendChild(item);\n        });\n    } catch (error) {\n        console.error('Error loading conversation...","File: frontend\u002fapp.js\u003cbr\u003eChunk: 4\u003cbr\u003ePreview: y-filter');\n\n\u002f\u002f Exercise event listeners\nexercisesButton.addEventListener('click', openExerciseModal);\ncloseExerciseModal.addEventListener('click', closeExerciseModalHandler);\nendExerciseButton.addEve...","File: frontend\u002fapp.js\u003cbr\u003eChunk: 5\u003cbr\u003ePreview: Duration: ${exercise.estimated_duration}\u003c\u002fp\u003e\n    `;\n    \n    card.addEventListener('click', () =\u003e startExercise(exercise.id));\n    \n    return card;\n}\n\nfunction filterExercises(category) {\n    const c...","File: frontend\u002fapp.js\u003cbr\u003eChunk: 6\u003cbr\u003ePreview: async function endCurrentExercise() {\n    if (!activeExerciseSession) return;\n    \n    if (!confirm('Are you sure you want to end this exercise?')) return;\n    \n    try {\n        const response = awai...","File: frontend\u002fapp.js\u003cbr\u003eChunk: 7\u003cbr\u003ePreview: addEventListener('mouseup', stopRecording);\nrecordButton.addEventListener('mouseleave', stopRecording);\n\n\u002f\u002f Touch events for mobile\nrecordButton.addEventListener('touchstart', (e) =\u003e {\n    e.preventDe..."],"x":[3.367032766342163,3.3578052520751953,2.909032106399536,2.3748769760131836,1.5432953834533691,1.8677157163619995,2.902700185775757,8.164095878601074],"y":[3.444121837615967,2.3110294342041016,2.379866123199463,3.543546199798584,5.462731838226318,4.869635581970215,4.386679172515869,-7.180001735687256],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#AB63FA","opacity":0.7,"size":8},"mode":"markers","name":"frontend\u002findex.html","text":["File: frontend\u002findex.html\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: \u003c!DOCTYPE html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n    \u003cmeta charset=\"UTF-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e\n    \u003ctitle\u003eVoice Assistant\u003c\u002ftitle\u003e\n    \u003clink rel=\"styleshe...","File: frontend\u002findex.html\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: \u003caside class=\"sidebar\"\u003e\n                \u003ch2\u003eConversations\u003c\u002fh2\u003e\n                \u003cdiv id=\"conversation-list\" class=\"conversation-list\"\u003e\n                    \u003cp class=\"loading\"\u003eLoading conversations...\u003c\u002fp...","File: frontend\u002findex.html\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: \u003c!-- Populated dynamically --\u003e\n            \u003c\u002fdiv\u003e\n        \u003c\u002fdiv\u003e\n    \u003c\u002fdiv\u003e\n    \n    \u003cscript src=\"audio-handler.js\"\u003e\u003c\u002fscript\u003e\n    \u003cscript src=\"app.js\"\u003e\u003c\u002fscript\u003e\n\u003c\u002fbody\u003e\n\u003c\u002fhtml\u003e"],"x":[4.441356658935547,4.95865535736084,5.966133117675781],"y":[4.458878040313721,3.9200856685638428,3.3999521732330322],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#FFA15A","opacity":0.7,"size":8},"mode":"markers","name":"frontend\u002flogin.html","text":["File: frontend\u002flogin.html\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: \u003c!DOCTYPE html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n    \u003cmeta charset=\"UTF-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e\n    \u003ctitle\u003eVoice Assistant - Login\u003c\u002ftitle\u003e\n    \u003clink rel=\"...","File: frontend\u002flogin.html\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: m\u003e\n        \u003c\u002fdiv\u003e\n    \u003c\u002fdiv\u003e\n    \n    \u003cscript\u003e\n        \u002f\u002f Check authentication status on page load\n        async function checkAuthStatus() {\n            try {\n                const response = await f...","File: frontend\u002flogin.html\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: document.querySelectorAll('.tab-button').forEach(b =\u003e b.classList.remove('active'));\n                button.classList.add('active');\n                \n                \u002f\u002f Show corresponding form\n       ...","File: frontend\u002flogin.html\u003cbr\u003eChunk: 3\u003cbr\u003ePreview: const password = document.getElementById('register-password').value;\n            const confirm = document.getElementById('register-confirm').value;\n            const errorEl = document.getElementById(..."],"x":[8.107026100158691,8.42811393737793,8.518868446350098,9.199231147766113],"y":[4.90416145324707,4.830014705657959,5.699010372161865,5.876298904418945],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#19D3F3","opacity":0.7,"size":8},"mode":"markers","name":"frontend\u002faudio-handler.js","text":["File: frontend\u002faudio-handler.js\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: class AudioHandler {\n    constructor(options) {\n        this.onDataAvailable = options.onDataAvailable;\n        this.onError = options.onError;\n        this.mediaRecorder = null;\n        this.audioChu...","File: frontend\u002faudio-handler.js\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: this.onError(event.error);\n            };\n            \n            \u002f\u002f Start recording\n            this.mediaRecorder.start();\n            \n        } catch (error) {\n            console.error('Error ac..."],"x":[7.676851749420166,7.725103378295898],"y":[-8.408942222595215,-8.406325340270996],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#FF6692","opacity":0.7,"size":8},"mode":"markers","name":"webhook\u002fhooks.json","text":["File: webhook\u002fhooks.json\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: [\n  {\n    \"id\": \"github-deployment\",\n    \"execute-command\": \"\u002fscripts\u002fdeploy.sh\",\n    \"command-working-directory\": \"\u002fapp\",\n    \"response-message\": \"Deployment started\",\n    \"response-headers\": [\n     ..."],"x":[10.038907051086426],"y":[-1.3055310249328613],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#B6E880","opacity":0.7,"size":8},"mode":"markers","name":"backend\u002ftts_instructor.py","text":["File: backend\u002ftts_instructor.py\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: import json\nimport logging\nfrom typing import List, Optional\nimport openai\nfrom models import Message\nfrom config import config\n\nlogger = logging.getLogger(__name__)\n\n\nclass TTSInstructor:\n    \"\"\"Gene...","File: backend\u002ftts_instructor.py\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: s}\\nAlways respect these preferences while adapting to the current context.\"\n        \n        system_prompt += \"\"\"\n\nConsider:\n- The emotional tone of the conversation (happy, serious, concerned, excit...","File: backend\u002ftts_instructor.py\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: ctions: {str(e)}\")\n            return None\n    \n    def analyze_preference_feedback(self, messages: List[Message], current_preferences: str) -\u003e Optional[str]:\n        \"\"\"\n        Analyze conversation ...","File: backend\u002ftts_instructor.py\u003cbr\u003eChunk: 3\u003cbr\u003ePreview: (f\"Updated voice preferences detected: {result}\")\n            return result\n            \n        except Exception as e:\n            logger.error(f\"Failed to analyze preference feedback: {str(e)}\")\n   ...","File: backend\u002ftts_instructor.py\u003cbr\u003eChunk: 4\u003cbr\u003ePreview: function\"]):\n            context = \"technical_support\"\n        elif len(recent_messages) == 1:\n            context = \"conversation_start\"\n        else:\n            context = \"general_conversation\"\n   ..."],"x":[-6.06396484375,-6.196681976318359,-6.640356540679932,-6.91986608505249,-7.154433250427246],"y":[-5.267618656158447,-5.917162895202637,-6.6159467697143555,-7.307278633117676,-8.314064979553223],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#FF97FF","opacity":0.7,"size":8},"mode":"markers","name":"backend\u002fstorage.py","text":["File: backend\u002fstorage.py\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: import json\nimport os\nimport fcntl\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom models import Conversation, ConversationSummary, Message\nfrom config import config\n\n\nclass JSONS...","File: backend\u002fstorage.py\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: ersations for a user\"\"\"\n        conversations_dir = os.path.join(config.DATA_STORAGE_PATH, \"conversations\")\n        summaries = []\n        \n        for filename in os.listdir(conversations_dir):\n     ...","File: backend\u002fstorage.py\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: conversation_id: str, \n        message: Message\n    ) -\u003e Optional[Conversation]:\n        \"\"\"Add a message to an existing conversation\"\"\"\n        conversation = JSONStorage.load_conversation(conversati..."],"x":[-7.039960861206055,-7.195557117462158,-7.613058567047119],"y":[2.8943231105804443,3.1726412773132324,3.952326774597168],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#FECB52","opacity":0.7,"size":8},"mode":"markers","name":"backend\u002frequirements.txt","text":["File: backend\u002frequirements.txt\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: fastapi==0.109.0\nuvicorn[standard]==0.27.0\npython-multipart==0.0.6\npython-jose[cryptography]==3.3.0\nPyJWT==2.10.1\nbcrypt==4.1.2\nopenai\u003e=1.95.0\npython-dotenv==1.0.0\npydantic==2.5.3\nwebsockets==12.0"],"x":[0.505314826965332],"y":[-3.4163413047790527],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#636EFA","opacity":0.7,"size":8},"mode":"markers","name":"backend\u002fauth.py","text":["File: backend\u002fauth.py\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: from datetime import datetime, timedelta\nfrom typing import Optional\nimport bcrypt\nimport jwt\nfrom fastapi import HTTPException, Depends, status\nfrom fastapi.security import HTTPBearer, HTTPAuthorizat...","File: backend\u002fauth.py\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: ken\n    return os.path.join(config.DATA_STORAGE_PATH, \"sessions\", f\"{safe_token}.json\")\n\n\ndef create_user(username: str, password: str) -\u003e User:\n    \"\"\"Create a new user\"\"\"\n    user_file = get_user_fi...","File: backend\u002fauth.py\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: redentials = Depends(security)) -\u003e User:\n    \"\"\"Get the current authenticated user from JWT token\"\"\"\n    token = credentials.credentials\n    \n    # Decode token\n    payload = decode_token(token)\n    u...","File: backend\u002fauth.py\u003cbr\u003eChunk: 3\u003cbr\u003ePreview: status_code=status.HTTP_404_NOT_FOUND,\n        detail=\"User not found\"\n    )\n\n\ndef logout_user(token: str):\n    \"\"\"Logout a user by removing their session\"\"\"\n    session_file = get_session_file_path(t..."],"x":[-4.88833475112915,-5.141034126281738,-5.986608028411865,-6.491789817810059],"y":[-0.6555991172790527,-0.47260409593582153,-0.5350617170333862,-0.23420709371566772],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#EF553B","opacity":0.7,"size":8},"mode":"markers","name":"backend\u002ftest_exercises_simp...","text":["File: backend\u002ftest_exercises_simple.py\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: #!\u002fusr\u002fbin\u002fenv python3\n\"\"\"\nSimple test to verify exercises are loaded correctly\n\"\"\"\nimport os\nimport sys\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\n\nfrom exercise_storage import Exerc..."],"x":[-11.070660591125488],"y":[0.16848228871822357],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#00CC96","opacity":0.7,"size":8},"mode":"markers","name":"backend\u002ftest_exercises.py","text":["File: backend\u002ftest_exercises.py\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: #!\u002fusr\u002fbin\u002fenv python3\n\"\"\"\nTest script to verify the exercise feature is working\n\"\"\"\nimport requests\nimport json\n\n# Base URL\nBASE_URL = \"http:\u002f\u002flocalhost:8001\"\n\n# Test credentials\nUSERNAME = \"test\"\nPA...","File: backend\u002ftest_exercises.py\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: se ended\")\n        else:\n            print(\"\u2705 No active exercise\")\n    \n    # Step 4: Start an exercise\n    if exercises:\n        exercise = exercises[0]  # Pick first exercise\n        print(f\"\\n4. St..."],"x":[-9.921546936035156,-9.822409629821777],"y":[0.7116110324859619,0.8703813552856445],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#AB63FA","opacity":0.7,"size":8},"mode":"markers","name":"backend\u002fexercise_storage.py","text":["File: backend\u002fexercise_storage.py\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: import json\nimport os\nfrom typing import List, Optional\nfrom models import Exercise, ExerciseSession\nfrom config import config\n\n\nclass ExerciseStorage:\n    \"\"\"Handle exercise-related storage operation...","File: backend\u002fexercise_storage.py\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: xists(session_file):\n            return None\n        \n        with open(session_file, 'r') as f:\n            session_data = json.load(f)\n            return ExerciseSession(**session_data)\n    \n    @st...","File: backend\u002fexercise_storage.py\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: conversation(conversation)\n        return conversation"],"x":[-5.75904655456543,-5.469096660614014,-2.329592704772949],"y":[2.1818995475769043,2.358041763305664,-10.184365272521973],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#FFA15A","opacity":0.7,"size":8},"mode":"markers","name":"backend\u002ftest_tts_instructor...","text":["File: backend\u002ftest_tts_instructor.py\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: #!\u002fusr\u002fbin\u002fenv python3\n\"\"\"\nTest script for TTS instruction generation\n\"\"\"\nimport asyncio\nfrom datetime import datetime\nfrom models import Message\nfrom tts_instructor import TTSInstructor\nfrom config i...","File: backend\u002ftest_tts_instructor.py\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: stant\", content=\"I understand presentations can be nerve-wracking. What aspect concerns you the most?\"),\n            Message(role=\"user\", content=\"I'm scared I'll forget what to say and everyone will ...","File: backend\u002ftest_tts_instructor.py\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: instructions = instructor.generate_tts_instructions(\n                scenario['messages'], \n                scenario['response']\n            )\n            \n            if instructions:\n               ..."],"x":[-11.196412086486816,-11.275664329528809,-11.920914649963379],"y":[-2.980790138244629,-3.6663269996643066,-4.0119218826293945],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#19D3F3","opacity":0.7,"size":8},"mode":"markers","name":"backend\u002fvoice_processor.py","text":["File: backend\u002fvoice_processor.py\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: import io\nimport os\nimport uuid\nfrom typing import Tuple, Optional\nimport openai\nfrom config import config\n\n\nclass VoiceProcessor:\n    \"\"\"Handle audio transcription and text-to-speech\"\"\"\n    \n    def ...","File: backend\u002fvoice_processor.py\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: cribe\"]:\n                stream = self.client.audio.transcriptions.create(\n                    model=config.OPENAI_STT_MODEL,\n                    file=audio_file,\n                    response_format=\"...","File: backend\u002fvoice_processor.py\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: tion(f\"Text-to-speech failed: {str(e)}\")\n    \n    def text_to_speech_stream(self, text: str, format: str = \"mp3\", instructions: Optional[str] = None):\n        \"\"\"Stream text-to-speech audio\"\"\"\n       ...","File: backend\u002fvoice_processor.py\u003cbr\u003eChunk: 3\u003cbr\u003ePreview: Return empty bytes if generation fails\n            return b\"\""],"x":[-3.2165637016296387,-3.7214128971099854,-4.145633697509766,-3.060394525527954],"y":[5.62075662612915,5.5912346839904785,5.380017280578613,-9.264780044555664],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#FF6692","opacity":0.7,"size":8},"mode":"markers","name":"backend\u002fai_handler.py","text":["File: backend\u002fai_handler.py\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: import json\nimport logging\nfrom typing import List, Dict, Any, Optional, Tuple\nimport openai\nfrom datetime import datetime\nfrom models import ToolCall, Message\nfrom config import config\nfrom tts_instr...","File: backend\u002fai_handler.py\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: \"type\": \"string\",\n                                \"description\": \"The mathematical expression to evaluate\"\n                            }\n                        },\n                        \"required\": ...","File: backend\u002fai_handler.py\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: break\n        \n        # Add regular messages (skip system messages in history)\n        for msg in conversation_history[-10:]:  # Last 10 messages for context\n            if msg.role != \"system\":\n    ...","File: backend\u002fai_handler.py\u003cbr\u003eChunk: 3\u003cbr\u003ePreview: ls:\n                # Execute tool calls\n                for tool_call in assistant_message.tool_calls:\n                    tool_name = tool_call.function.name\n                    arguments = json.loa...","File: backend\u002fai_handler.py\u003cbr\u003eChunk: 4\u003cbr\u003ePreview: hoices[0].message.content\n            else:\n                # No tools needed, use the initial response\n                response_text = assistant_message.content\n            \n            # Generate TT...","File: backend\u002fai_handler.py\u003cbr\u003eChunk: 5\u003cbr\u003ePreview: str(e)}\"\n            return error_message, tool_calls_made, None"],"x":[-3.276066303253174,-2.814004421234131,-2.506415605545044,-3.178281545639038,-3.3937528133392334,-3.1711854934692383],"y":[-4.123621940612793,-5.163634777069092,-5.600228309631348,-6.272854804992676,-6.747796535491943,-8.492761611938477],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#B6E880","opacity":0.7,"size":8},"mode":"markers","name":"backend\u002ftest_preference_lea...","text":["File: backend\u002ftest_preference_learning.py\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: #!\u002fusr\u002fbin\u002fenv python3\n\"\"\"\nTest script for automatic TTS preference learning\n\"\"\"\nimport asyncio\nfrom datetime import datetime\nfrom models import Message, User\nfrom tts_instructor import TTSInstructor\n...","File: backend\u002ftest_preference_learning.py\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: , content=\"I'm really stressed about my presentation\"),\n            Message(role=\"assistant\", content=\"I understand presentations can be nerve-wracking.\", \n                   tts_instructions=\"Speak e...","File: backend\u002ftest_preference_learning.py\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: n (update expected: {scenario['expected_update']})\")\n        else:\n            print(f\"\u2717 Result doesn't match expectation (update expected: {scenario['expected_update']})\")\n        \n        if updated...","File: backend\u002ftest_preference_learning.py\u003cbr\u003eChunk: 3\u003cbr\u003ePreview: ing what would happen\n    \n    print(\"\\n\u2705 Integration test complete!\")\n\nif __name__ == \"__main__\":\n    # Run preference learning test\n    test_preference_learning()\n    \n    # Run integration test\n   ..."],"x":[-10.248445510864258,-9.988775253295898,-9.340356826782227,-9.439319610595703],"y":[-3.0716300010681152,-3.689072370529175,-4.1599555015563965,-5.134571552276611],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#FF97FF","opacity":0.7,"size":8},"mode":"markers","name":"backend\u002fconfig.py","text":["File: backend\u002fconfig.py\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\nclass Config:\n    # OpenAI Configuration (for STT and TTS)\n    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n    \n    # Speech-to-Text Configur...","File: backend\u002fconfig.py\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: _PATH = os.getenv(\"DATA_STORAGE_PATH\", \"\u002fapp\u002fdata\")\n    \n    # Audio Configuration\n    MAX_AUDIO_LENGTH_SECONDS = int(os.getenv(\"MAX_AUDIO_LENGTH_SECONDS\", \"60\"))\n    AUDIO_CLEANUP_HOURS = int(os.gete..."],"x":[4.511519908905029,4.631760597229004],"y":[-5.2874956130981445,-5.3854217529296875],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#FECB52","opacity":0.7,"size":8},"mode":"markers","name":"backend\u002fmodels.py","text":["File: backend\u002fmodels.py\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: from pydantic import BaseModel, Field\nfrom datetime import datetime\nfrom typing import List, Optional, Literal, Any\nimport uuid\n\n\nclass User(BaseModel):\n    id: str = Field(default_factory=lambda: str...","File: backend\u002fmodels.py\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: e: str\n    description: str  # User-facing description\n    category: str  # business, creative, technical, leadership\n    difficulty: str  # beginner, intermediate, advanced\n    system_prompt: str  # ..."],"x":[-2.205169439315796,-2.009445905685425],"y":[-1.795465350151062,3.031897783279419],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#636EFA","opacity":0.7,"size":8},"mode":"markers","name":"backend\u002ftest_persistence.py","text":["File: backend\u002ftest_persistence.py\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: #!\u002fusr\u002fbin\u002fenv python3\n\"\"\"\nTest script to verify voice preference persistence between sessions\n\"\"\"\nimport json\nimport os\nfrom datetime import datetime\nfrom auth import create_user, authenticate_user, ...","File: backend\u002ftest_persistence.py\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: rences}\")\n        print(f\"   Preference updated at: {authenticated_user.voice_preference_updated}\")\n        \n        # Verify preferences match\n        if authenticated_user.voice_preferences == new_p...","File: backend\u002ftest_persistence.py\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: ferences\")\n        else:\n            print(\"\u2705 User already has custom preferences\")\n    else:\n        print(\"\u274c Could not authenticate test user\")\n\nif __name__ == \"__main__\":\n    test_voice_preference_..."],"x":[-8.026719093322754,-7.968033313751221,-7.815551280975342],"y":[-1.7388583421707153,-2.3688735961914062,-2.87029767036438],"type":"scatter"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"#EF553B","opacity":0.7,"size":8},"mode":"markers","name":"backend\u002fapp.py","text":["File: backend\u002fapp.py\u003cbr\u003eChunk: 0\u003cbr\u003ePreview: from fastapi import FastAPI, Depends, HTTPException, UploadFile, File, status\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import FileResponse\nfrom fastapi.staticfiles imp...","File: backend\u002fapp.py\u003cbr\u003eChunk: 1\u003cbr\u003ePreview: ise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Registration is disabled in single user mode\"\n        )\n    \n    try:\n        user = create_user(request.usern...","File: backend\u002fapp.py\u003cbr\u003eChunk: 2\u003cbr\u003ePreview: file too large\"\n            )\n        \n        # Transcribe audio\n        transcription = voice_processor.transcribe_audio(contents, format=\"webm\")\n        \n        # Get or create conversation\n      ...","File: backend\u002fapp.py\u003cbr\u003eChunk: 3\u003cbr\u003ePreview: raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=str(e)\n        )\n\n\n# Conversation history endpoints\n@app.get(\"\u002fapi\u002fconversations\", response_model...","File: backend\u002fapp.py\u003cbr\u003eChunk: 4\u003cbr\u003ePreview: detail=\"Exercise not found\"\n        )\n    \n    # Create new conversation with exercise system prompt\n    conversation = ExerciseStorage.create_exercise_conversation(current_user.id, exercise)\n    \n   ...","File: backend\u002fapp.py\u003cbr\u003eChunk: 5\u003cbr\u003ePreview: datetime.utcnow()\n    session.status = \"completed\"\n    ExerciseStorage.save_exercise_session(session)\n    \n    return {\"status\": \"completed\", \"session_id\": session_id}\n\n\n@app.get(\"\u002fapi\u002fexercises\u002factiv..."],"x":[-2.3635690212249756,-2.4251515865325928,-1.1375744342803955,-2.1030139923095703,-2.3244552612304688,-3.3563759326934814],"y":[-0.09354884922504425,-0.5352911353111267,-0.37884899973869324,1.1262298822402954,1.4605027437210083,1.5146734714508057],"type":"scatter"}],                        {"template":{"data":{"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scattermap":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermap"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"title":{"text":"Document Embeddings t-SNE Visualization (116 chunks)"},"xaxis":{"title":{"text":"t-SNE Component 1"}},"yaxis":{"title":{"text":"t-SNE Component 2"}},"hovermode":"closest","width":1200,"height":800,"legend":{"yanchor":"top","y":0.99,"xanchor":"left","x":1.01},"clickmode":"event+select","showlegend":true},                        {"displayModeBar": true, "displaylogo": false, "modeBarButtonsToRemove": ["pan2d", "lasso2d", "select2d"], "responsive": true}                    )                };            </script>        </div>
</body>
</html>
        </body>
        </html>
        